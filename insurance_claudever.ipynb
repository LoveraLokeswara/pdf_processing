{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in ./.venv/lib/python3.9/site-packages (0.46.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.9/site-packages (from anthropic) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.9/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from anthropic) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.9/site-packages (from anthropic) (2.10.6)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in ./.venv/lib/python3.9/site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->anthropic) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in ./.venv/lib/python3.9/site-packages (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in ./.venv/lib/python3.9/site-packages (from PyPDF2) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.9/site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in ./.venv/lib/python3.9/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./.venv/lib/python3.9/site-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from faiss-cpu) (24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install anthropic\n",
    "!pip install PyPDF2\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEWWWW\n",
    "def load_pdf_with_ocr(file_path):\n",
    "    \"\"\"\n",
    "    Load PDF using both PyPDF2 and Pytesseract for better text extraction\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    \n",
    "    # First try PyPDF2 extraction\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num, page in enumerate(reader.pages, 1):\n",
    "            # Try regular text extraction first\n",
    "            page_text = page.extract_text()\n",
    "            \n",
    "            # If page text is empty or contains suspected table content (pages 41+)\n",
    "            if not page_text.strip() or page_num >= 41:\n",
    "                # Convert PDF page to image for OCR\n",
    "                images = convert_from_path(file_path, first_page=page_num, last_page=page_num)\n",
    "                for img in images:\n",
    "                    # Use OCR to extract text\n",
    "                    ocr_text = pytesseract.image_to_string(img, lang='eng')\n",
    "                    text += f\"\\n=== Page {page_num} OCR Text ===\\n{ocr_text}\"\n",
    "            else:\n",
    "                text += f\"\\n=== Page {page_num} PDF Text ===\\n{page_text}\"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### VERSION LAMA (GAADA SPECIAL HANDLING OF PAGE 41 ONWARDS)\n",
    "\n",
    "# def create_chunks(text, chunk_size=2000, overlap=400):\n",
    "#     \"\"\"\n",
    "#     Create overlapping chunks of text with special handling for table data\n",
    "#     \"\"\"\n",
    "#     chunks = []\n",
    "#     lines = text.split('\\n')\n",
    "#     current_chunk = []\n",
    "#     current_length = 0\n",
    "    \n",
    "#     for line in lines:\n",
    "#         # Special handling for table content (marked by OCR)\n",
    "#         if \"=== Page\" in line and \"OCR Text\" in line:\n",
    "#             # If we have a current chunk, add it to chunks\n",
    "#             if current_chunk:\n",
    "#                 chunks.append('\\n'.join(current_chunk))\n",
    "#             # Start a new chunk with this table\n",
    "#             current_chunk = [line]\n",
    "#             current_length = len(line)\n",
    "#         else:\n",
    "#             line_length = len(line)\n",
    "#             if current_length + line_length > chunk_size:\n",
    "#                 # Add current chunk to chunks\n",
    "#                 chunks.append('\\n'.join(current_chunk))\n",
    "#                 # Start new chunk with overlap\n",
    "#                 overlap_start = max(0, len(current_chunk) - overlap)\n",
    "#                 current_chunk = current_chunk[overlap_start:] + [line]\n",
    "#                 current_length = sum(len(l) for l in current_chunk)\n",
    "#             else:\n",
    "#                 current_chunk.append(line)\n",
    "#                 current_length += line_length\n",
    "    \n",
    "#     # Add the last chunk if it exists\n",
    "#     if current_chunk:\n",
    "#         chunks.append('\\n'.join(current_chunk))\n",
    "    \n",
    "#     return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text, chunk_size=2000, overlap=400):\n",
    "    \"\"\"\n",
    "    Create larger overlapping chunks of text with special handling for table data\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    lines = text.split('\\n')\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        # Special handling for pages 41 onwards (where specific amounts are)\n",
    "        if \"=== Page\" in line:\n",
    "            page_num = int(line.split(\"Page\")[1].split()[0])\n",
    "            if page_num >= 41:\n",
    "                # If we have a current chunk, add it to chunks\n",
    "                if current_chunk:\n",
    "                    chunks.append('\\n'.join(current_chunk))\n",
    "                # Start a new chunk with this table, using smaller chunks for detailed info\n",
    "                current_chunk = [line]\n",
    "                current_length = len(line)\n",
    "                chunk_size = 1000  # Smaller chunks for detailed sections\n",
    "            else:\n",
    "                chunk_size = 2000  # Larger chunks for general content\n",
    "        \n",
    "        line_length = len(line)\n",
    "        if current_length + line_length > chunk_size:\n",
    "            # Add current chunk to chunks\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "            # Start new chunk with overlap\n",
    "            overlap_start = max(0, len(current_chunk) - overlap)\n",
    "            current_chunk = current_chunk[overlap_start:] + [line]\n",
    "            current_length = sum(len(l) for l in current_chunk)\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "            current_length += line_length\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### without OCR\n",
    "\n",
    "# def load_pdf(file_path):\n",
    "#     with open(file_path, 'rb') as file:\n",
    "#         reader = PyPDF2.PdfReader(file)\n",
    "#         text = \"\"\n",
    "#         for page in reader.pages:\n",
    "#             text += page.extract_text()\n",
    "#     return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(text):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    chunks = create_chunks(text)\n",
    "    chunk_embeddings = model.encode(chunks)\n",
    "    # sentences = text.split('. ')\n",
    "    # sentence_embeddings = model.encode(sentences)\n",
    "    dimension = chunk_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(chunk_embeddings))\n",
    "    return index, chunks, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(index)\n",
    "# print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert insurance policy assistant analyzing a health insurance policy document. Your role is to provide precise answers about coverage, amounts, and deductibles based SOLELY on the provided document context.\n",
    "FORMAT REQUIREMENTS:\n",
    "Always structure your response exactly as follows, with each section on a new line:\n",
    "Is it covered: [MUST be \"Yes\", \"No\", or \"No information found\"]\n",
    "How much is covered: [MUST be a numerical amount with currency symbol (USD) or \"No information found\" or a range of the amount with currency symbol (USD)]\n",
    "Deductible: [MUST be a numerical amount with currency symbol (USD) or \"No information found\" or a range of the amount with currency symbol (USD) or a selection of options based on the information inside the PDF]\n",
    "Exact excerpt: [Direct quote from the document that supports your answer]\n",
    "\n",
    "CRITICAL RULES:\n",
    "AMOUNTS AND COVERAGE:\n",
    "For coverage amounts, ALWAYS check pages 41 onwards first\n",
    "Only provide numerical amounts that are explicitly stated in the document\n",
    "Never infer or calculate amounts not directly stated\n",
    "It is fine to include a range (just specify \"Between ... - ...\" depending on the information available in the file (specifically in the last few pages of the document that you have access to))\n",
    "\n",
    "\n",
    "DOCUMENT SECTIONS TO CHECK:\n",
    "Benefits section for coverage details\n",
    "text after \"Annexures I & II\" have the specific amounts or a range amount\n",
    "Annexures I & II for exclusions and sub-limits\n",
    "General Exclusions section for non-covered items\n",
    "\n",
    "\n",
    "RESPONSE RULES:\n",
    "NEVER use external knowledge or make assumptions\n",
    "If information is not found in the provided context, respond with \"No information found\" for that section\n",
    "Always include word-for-word quotes from the document in the \"Exact excerpt\" section\n",
    "If multiple relevant excerpts exist, prioritize the one with specific amounts\n",
    "For any \"Yes\" coverage answer, you MUST provide supporting evidence in the exact excerpt\n",
    "\n",
    "\n",
    "ACCURACY REQUIREMENTS:\n",
    "Only answer \"Yes\" to coverage if explicitly stated in the document\n",
    "Only include amounts that are specifically mentioned. It is okay to come up with a range (specify the lower bound and upper bound of the sum insured or deductibles)\n",
    "If there are conditions or limitations, include these in the exact excerpt\n",
    "If the coverage has multiple tiers or conditions, include all relevant information in the exact excerpt\n",
    "\n",
    "Example response format:\n",
    "Is it covered: Yes\n",
    "How much is covered: Between USD 200 - USD 1,000 in multiples of 100\n",
    "Deductible: Either Nil / USD 50 / USD 100 / USD 200 \n",
    "Exact excerpt: \"Emergency dental treatment are covered up to USD 10,000 with a deductible of Nil / USD 50 / USD 100 / USD 200 per claim.\"\"\"\n",
    "\n",
    "\n",
    "alternative_system_prompt=\"\"\"You are an insurance policy assistant. Answer questions ONLY based on the provided policy document context. Format your response exactly as follows:\n",
    "                Is it covered: [Yes/No/Partial]\n",
    "                Coverage amount: [Extract exact amount from document or \"No specific amount mentioned\"]\n",
    "                Deductible: [Extract exact deductible or \"No deductible mentioned\"]\n",
    "                Source: [Exact quote from the document supporting your answer]\n",
    "\n",
    "                Important rules:\n",
    "                1. ONLY use information from the provided context\n",
    "                2. If information is not found, respond with \"No information found\" for each section\n",
    "                3. For coverage amounts, ALWAYS check the tables section first\n",
    "                4. Never make assumptions or use external knowledge\n",
    "                5. Always include exact quotes from the document\n",
    "                6. Be precise about coverage amounts and conditions\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "pdf_text = load_pdf_with_ocr('NBHTGBP22011V012223.pdf')\n",
    "# print(pdf_text)\n",
    "index, sentences, model = create_faiss_index(pdf_text)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### VERSION LAMA (GAADA SPECIAL HANDLING OF PAGE 41 ONWARDS)\n",
    "# \n",
    "# def query_faiss_index(query, index, sentences, model, k=3):\n",
    "#     query_embedding = model.encode([query])\n",
    "#     distances, indices = index.search(np.array(query_embedding), k)\n",
    "#     relevant_sentences = [sentences[i] for i in indices[0]]\n",
    "#     return relevant_sentences\n",
    "\n",
    "# def rag_chatbot(query):\n",
    "#     relevant_sentences = query_faiss_index(query, index, sentences, model)\n",
    "#     context = \" \".join(relevant_sentences)\n",
    "    \n",
    "#     client = anthropic.Anthropic(api_key=API_KEY)\n",
    "    \n",
    "\n",
    "#     response = client.messages.create(\n",
    "#         model=\"claude-3-opus-20240229\",  # Use the latest Claude model\n",
    "#         max_tokens=1000,\n",
    "#         system=system_prompt,\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\", \n",
    "#                 \"content\": f\"Context:\\n{context}\\n\\nQuestion:\\n{query}\"}\n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     return response.content[0].text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_faiss_index(query, index, sentences, model, k=5):\n",
    "    \"\"\"\n",
    "    Enhanced query function that prioritizes content from later pages\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    # Get more candidates initially\n",
    "    k_initial = min(k * 2, len(sentences))\n",
    "    distances, indices = index.search(np.array(query_embedding), k_initial)\n",
    "    \n",
    "    # Prioritize chunks from pages 41 onwards\n",
    "    relevant_chunks = []\n",
    "    late_page_chunks = []\n",
    "    early_page_chunks = []\n",
    "    \n",
    "    for idx in indices[0]:\n",
    "        chunk = sentences[idx]\n",
    "        # Check if chunk contains page number\n",
    "        if \"=== Page\" in chunk:\n",
    "            page_num = int(chunk.split(\"Page\")[1].split()[0])\n",
    "            if page_num >= 41:\n",
    "                late_page_chunks.append(chunk)\n",
    "            else:\n",
    "                early_page_chunks.append(chunk)\n",
    "        else:\n",
    "            early_page_chunks.append(chunk)\n",
    "    \n",
    "    # Combine chunks, prioritizing later pages\n",
    "    relevant_chunks = late_page_chunks + early_page_chunks\n",
    "    return relevant_chunks[:k]\n",
    "\n",
    "def rag_chatbot(query):\n",
    "    \"\"\"\n",
    "    Enhanced RAG chatbot that includes more context and prioritizes specific information\n",
    "    \"\"\"\n",
    "    relevant_chunks = query_faiss_index(query, index, sentences, model, k=5)\n",
    "    \n",
    "    # Add specific prompt to look for detailed information\n",
    "    enhanced_query = f\"\"\"Please focus particularly on specific amounts and coverage details, \n",
    "    especially from pages 41 onwards in the document.\n",
    "    \n",
    "    Context:\n",
    "    {' '.join(relevant_chunks)}\n",
    "    \n",
    "    Question:\n",
    "    {query}\"\"\"\n",
    "    \n",
    "    client = anthropic.Anthropic(api_key=API_KEY)\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": enhanced_query\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could not find any specific information in the provided policy document context about coverage for a trip delay of 45 hours. The policy mentions coverage for Trip Delay in section 3.15, but it does not specify the exact number of hours of delay or the corresponding coverage amount. The excerpt only states:\n",
      "\n",
      "\"If departure of your scheduled common carrier is delayed beyond a specified number of hours due to any of the \n",
      "following reasons, then we will pay an amount as mentioned in certificate of insurance for every block of hours of delay \n",
      "(as mentioned in certificate of insurance) maximum up to 24 hours of delay\"\n",
      "\n",
      "Without the certificate of insurance which likely contains the specific delay hours and coverage amounts, I do not have enough information to determine the exact trip delay coverage for 45 hours based solely on the provided policy wordings. More context from the certificate of insurance would be needed to give a precise answer.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"My trip was delayed and I paid 45, how much am I covered for?\"\n",
    "response = rag_chatbot(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Insurance Agent Chatbot! Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: My trip was delayed and I paid 45, how much am I covered for?\n",
      "Bot: I could not find any specific information in the provided policy document context about coverage for a trip delay of 45 hours. The policy mentions coverage for Trip Delay in section 3.15, but it does not specify the exact number of hours of delay or the corresponding coverage amount. The relevant excerpt is:\n",
      "\n",
      "\"If departure of your scheduled common carrier is delayed beyond a specified number of hours due to any of the following reasons, then we will pay an amount as mentioned in certificate of insurance for every block of hours of delay (as mentioned in certificate of insurance) maximum up to 24 hours of delay\"\n",
      "\n",
      "Without the certificate of insurance which likely contains the specific delay hours and coverage amounts, I do not have enough information to determine the exact trip delay coverage for 45 hours.\n",
      "\n",
      "In summary:\n",
      "Is it covered: No information found\n",
      "How much is covered: No information found \n",
      "Deductible: No information found\n",
      "Exact excerpt: \"If departure of your scheduled common carrier is delayed beyond a specified number of hours due to any of the following reasons, then we will pay an amount as mentioned in certificate of insurance for every block of hours of delay (as mentioned in certificate of insurance) maximum up to 24 hours of delay\"\n",
      "============================\n",
      "You: exit\n"
     ]
    }
   ],
   "source": [
    "def chatbot_interface():\n",
    "    print(\"Welcome to the Insurance Agent Chatbot! Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        query = input(\"Welcome to the Insurance Agent Chatbot! Type 'exit' to quit.\")\n",
    "        print(f\"You: {query}\")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "        response = rag_chatbot(query)\n",
    "        print(f\"Bot: {response}\")\n",
    "        print('============================')\n",
    "\n",
    "chatbot_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information provided in the context, there is no clear answer to your specific question about trip delay coverage of $45. The excerpt discusses trip cancellation coverage, but does not mention any details about trip delay coverage or amounts. It states that any claim amount already paid under \"Delay of Checked-in Baggage\" cover would be deducted from the trip cancellation claim amount, but no specific dollar figures are provided. Without more details from the policy document, I do not have enough information to determine how much, if any, coverage would be provided for a $45 trip delay expense.\n",
    "\n",
    "\n",
    "\n",
    "Based on the information provided in the policy document, here are the details for Trip Delay coverage:\n",
    "\n",
    "Is it covered: Yes\n",
    "How much is covered: No specific amount mentioned\n",
    "Deductible: No information found\n",
    "Exact excerpt: \"If departure of your scheduled common carrier is delayed beyond a specified number of hours due to any of the \n",
    "following reasons, then we will pay an amount as mentioned in certificate of insurance for every block of hours of delay \n",
    "(as mentioned in certificate of insurance) maximum up to 24 hours of delay\"\n",
    "\n",
    "The excerpt indicates trip delay is covered, but does not specify the exact amount. It states an amount is payable as mentioned in the certificate of insurance for each block of hours delayed, up to a maximum of 24 hours. However, the excerpt does not provide the specific compensation amounts or any deductible that may apply. More details would need to be checked in the certificate of insurance to determine the exact coverage amount and deductible, if any, for trip delay.\n",
    "\n",
    "######\n",
    "I could not find any specific information in the provided policy document about the amount of coverage for trip delay of 45 hours. The policy does mention trip delay coverage in section 3.15, but it does not specify the exact number of hours of delay or the corresponding claim amounts. \n",
    "\n",
    "Here are the relevant details I found:\n",
    "\n",
    "Is it covered: No information found\n",
    "How much is covered: No information found\n",
    "Deductible: No information found\n",
    "Exact excerpt: \"If departure of your scheduled common carrier is delayed beyond a specified number of hours due to any of the following reasons, then we will pay an amount as mentioned in certificate of insurance for every block of hours of delay (as mentioned in certificate of insurance) maximum up to 24 hours of delay:\"\n",
    "\n",
    "The excerpt indicates there is some coverage for trip delay, but the number of hours and claim amounts would be specified in the certificate of insurance, which is not provided here. More information from the certificate of insurance would be needed to determine the exact trip delay coverage for a 45 hour delay.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
